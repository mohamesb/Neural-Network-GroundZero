{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def _init_(self, input_size= 784, hidden_layers = [512,512], output_size=10):\n",
    "        self.input_size = input_size\n",
    "        self.hidden_layers = hidden_layers\n",
    "        self.outputsize = output_size\n",
    "        self.weights = []\n",
    "        self.bias = []\n",
    "\n",
    "        #Input to Hidden layers netowrk\n",
    "        self.weights.append(0.01 * np.random.rand(input_size, hidden_layers[0]))\n",
    "        self.bias.append(np.zeros((1, hidden_layers[0])))\n",
    "\n",
    "        #Hidden layers network\n",
    "        for i in range(len(hidden_layers)-1):\n",
    "            self.weights.append(hidden_layers[i], hidden_layers[i+1])\n",
    "            self.bias.append(np.zeros(1, hidden_layers[i+1]))\n",
    "        \n",
    "        #Hidden layers network to output\n",
    "        self.weights.append(0.01 * np.random.rand(len(hidden_layers)-1, output_size))\n",
    "        self.bias.append(np.zeros(1,output_size))\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        layers = [inputs]\n",
    "        for i in range(len(self.weights)):\n",
    "            layers.append(np.dot(layers[-1],self.weights[i] + self.bias[i]))\n",
    "        return layers[-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Activation Function - Sigmoid Function\n",
    "def sigmoid(x):\n",
    "    return(1/(1 + np.exp(-x)))\n",
    "\n",
    "#Feed Forward Neural Network\n",
    "def feed_forward(x, w1, w2):\n",
    "    z1 = x.dot(w1)\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = x.dot(w2)\n",
    "    a2 = sigmoid(z2)\n",
    "    return (a2)\n",
    "\n",
    "#Weight initialization\n",
    "\n",
    "def weight_int(x,y):\n",
    "    li = []\n",
    "    for i in range(x*y):\n",
    "        li.append(np.random.rand())\n",
    "    return np.array(li).reshape(x,y)\n",
    "\n",
    "#Loss function - RMSE\n",
    "def loss_function(out, Y):\n",
    "    s =(np.square(out-Y))\n",
    "    d = np.sum(s)/len(y)\n",
    "    v = np.sqrt(d)\n",
    "    return (v)\n",
    "\n",
    "def backpropagation(x,y,w1,w2, alpha):\n",
    "    # Hidden Layer\n",
    "    z1 = x.dot(w1)\n",
    "    a1 = sigmoid(z1)\n",
    "    z2 = x.dot(w2)\n",
    "    a2 = sigmoid(z2)\n",
    "\n",
    "    #Error in Output Layer\n",
    "    d2 = a2 - y\n",
    "    d1 = np.multiply((w2.dot((d2.transpose()))).transpose(), \n",
    "\t\t\t\t\t\t\t\t(np.multiply(a1, 1-a1)))\n",
    "    \n",
    "    #Gradient for w1 and w2\n",
    "    w1_adj = x.transpose().dot(d1)\n",
    "    w2_adj = a1.transpose().dot(d2)\n",
    "\n",
    "    w1 = w1-(alpha*(w1_adj))\n",
    "    w2 = w2-(alpha*(w2_adj))\n",
    "\n",
    "    return (w1,w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = weight_int(30, 5)\n",
    "w2 = weight_int(5, 3)\n",
    "\n",
    "print(w1, \"\\n\\n\", w2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(x, Y, w1, w2, alpha = 0.01, epoch = 10):\n",
    "\tacc =[]\n",
    "\tlosss =[]\n",
    "\tfor j in range(epoch):\n",
    "\t\tl =[]\n",
    "\t\tfor i in range(len(x)):\n",
    "\t\t\tout = feed_forward(x[i], w1, w2)\n",
    "\t\t\tl.append((loss_function(out, Y[i])))\n",
    "\t\t\tw1, w2 = backpropagation(x[i], y[i], w1, w2, alpha)\n",
    "\t\tprint(\"epochs:\", j + 1, \"======== acc:\", (1-(sum(l)/len(x)))*100) \n",
    "\t\tacc.append((1-(sum(l)/len(x)))*100)\n",
    "\t\tlosss.append(sum(l)/len(x))\n",
    "\treturn(acc, losss, w1, w2)\n",
    "\n",
    "acc, losss, w1, w2 = train(x, y, w1, w2, 0.1, 100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
